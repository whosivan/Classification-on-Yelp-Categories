{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROJECT MCMULTY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AWS MySQL RDS Contents:\n",
    "- business.db (30M)\n",
    "- tip.db (141M)\n",
    "- review.db (3.5G)\n",
    "- checkin.db (4.8M)\n",
    "- business_hours.db (13M)\n",
    "- user.db (1.3G)\n",
    "\n",
    "<center>**Click below for all other data information** </center>\n",
    "[![](https://s3-media2.fl.yelpcdn.com/assets/srv0/styleguide/1ea40efd80f5/assets/img/brand_guidelines/yelp_fullcolor.png)](https://www.yelp.com/dataset/documentation/json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "1. [AWS RDS Connection](#0)\n",
    "2. [Import Data](#2)<br>\n",
    "     2.1 [Read yelp data from RDS](#2.1)<br>\n",
    "     2.2 [Read supplement data from csv and convert all sub-categories to primary categories](#2.2)<br>\n",
    "3. [Exploratory Data Analysis](#3)<br>\n",
    "     3.1 [Visualize star rating distribution](#3.1)<br>\n",
    "     3.2 [Visualize category distribution](#3.2)<br>\n",
    "     3.3 [Visualize Review Counts by City Distribution](#3.3)<br>\n",
    "4. [Data Cleaning](#4)<br>\n",
    "5. [Feature Engineering](#4)<br>\n",
    "     5.1 [Calculate number of business days within a week](#5.1)<br>\n",
    "     5.2 [Calculate number of business hours within a week](#5.2)<br>\n",
    "     5.3 [Extract the length of the business unit](#5.3)<br>\n",
    "     5.4 [Extract number of vowels in the business name](#5.4)<br>\n",
    "     5.5 [Extract number of friends on Yelp](#5.5)<br>\n",
    "     5.6 [Final data cleaning prior to modeling](#5.6)<br>\n",
    "6. [Baseline Model](#6)<br>\n",
    "     6.1 [Define classification metrics function & downcasting function](#6.1)<br>\n",
    "     6.2 [Baseline Logistic Regression model (One feature)](#6.2)<br>\n",
    "     6.3 [Expanding Logistic Regression with 5 original features](#6.3)<br>\n",
    "     6.4 [Expanding Logistic Regression with 5 feature engineered features](#6.4)<br>\n",
    "     6.5 [Expanding Logistic Regression with 10 features](#6.5)<br>\n",
    "     6.6 [Logistic Regression Model (All features)](#6.6)<br>\n",
    "     6.7 [Baseline RF model (One feature)](#6.7)<br>\n",
    "     6.8 [Baseline xgBoost model (One feature)](#6.8)<br>\n",
    "     6.9 [RF model (All features)](#6.9)<br>\n",
    "     6.10 [xgBoost model (All feature)](#6.10)<br>\n",
    "7. [Natural Language Processing (NLP) as additional features](#7)<br>\n",
    "     7.1 [TFIDFVectorizer](#7.1)<br>\n",
    "     7.2 [RF Model (TFIDF + all features)](#7.2)<br>\n",
    "     7.3 [xgBoost model (TFIDF + all features)](#7.3)<br>\n",
    "8. [Model Optimization](#8)<br>\n",
    "9. [Final test-scores](#9)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import mysql.connector as sql\n",
    "import math\n",
    "import itertools\n",
    "from functools import reduce\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from itertools import groupby\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=0></a>1. Define AWS RDS endpoint connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"url\"><center><img src=\"https://github.com/whosivan/sf18_ds9/blob/master/student_submissions/projects/luther/Cui_Ivan/AWS_setup.png?raw=true\" align=\"left\" height=\"480\" width=\"480\" ></center></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db_connection = sql.connect(host = 'yelpinstance.cvchd2jvtnxy.us-east-1.rds.amazonaws.com', \n",
    "                            database = 'yelpdb', \n",
    "                            user = 'root', \n",
    "                            password = 'admin123')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=2></a> 2. Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id=2.1></a> 2.1 Read yelp data from RDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time\n",
    "business_df = pd.read_sql('''\n",
    "select b.business_id, b.name as business_name, b.city, b.state, b.stars as business_stars, b.review_count, b.is_open, b.categories, bh.monday, bh.tuesday, bh.wednesday, bh.thursday, bh.friday, bh.saturday, bh.sunday\n",
    "from business as b\n",
    "join business_hours as bh\n",
    "on b.business_id = bh.business_id\n",
    "''', con = db_connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time\n",
    "review_df = pd.read_sql('''SELECT user_id,\n",
    "                                  business_id,\n",
    "                                  stars,\n",
    "                                  text as review_text,\n",
    "                                  useful as r_review,\n",
    "                                  funny as r_funny, \n",
    "                                  cool as r_cool FROM review LIMIT 1000000''', con = db_connection)\n",
    "tip_df = pd.read_sql('''SELECT business_id,\n",
    "                               count(text) as tip_count FROM tip\n",
    "                               group by 1''', con = db_connection)\n",
    "checkin_df = pd.read_sql('''SELECT business_id,\n",
    "                                   checkins FROM checkin''', con = db_connection)\n",
    "user_df = pd.read_sql('''SELECT user_id,\n",
    "                                name as user_name,\n",
    "                                review_count as user_review_counts,\n",
    "                                friends,\n",
    "                                useful as u_useful,\n",
    "                                funny as u_funny,\n",
    "                                cool as u_cool,\n",
    "                                fans as u_fans,\n",
    "                                average_stars,\n",
    "                                compliment_cute,\n",
    "                                compliment_more,\n",
    "                                compliment_profile,\n",
    "                                compliment_cute,\n",
    "                                compliment_list,\n",
    "                                compliment_note,\n",
    "                                compliment_plain,\n",
    "                                compliment_cool,\n",
    "                                compliment_funny,\n",
    "                                compliment_writer,\n",
    "                                compliment_photos FROM user''', con = db_connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id=2.2></a> 2.2 Read supplement data from csv and convert all sub-categories to primary categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "category_list = pd.read_csv('../yelp_data/yelp-business-categories.csv')\n",
    "category_list.drop_duplicates(subset='Sub-Categories', keep=\"first\", inplace=True)\n",
    "\n",
    "def pd_to_dict(df):\n",
    "    category_dict = {k: g[\"Sub-Categories\"].tolist() for k,g in df.groupby(\"Primary_Categories\")}\n",
    "    category_dict = dict((v,k) for k in category_dict for v in category_dict[k])\n",
    "    return category_dict\n",
    "\n",
    "category_dict = pd_to_dict(category_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_primary_category_label(df):\n",
    "    df.categories = df.categories.apply(lambda x:x.rstrip('\\r')).str[0:].str.split(';')\n",
    "    df.categories = df.categories.apply(lambda col: col[0])\n",
    "    return df\n",
    "\n",
    "get_primary_category_label(business_df).head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def map_main_category(df, dict_):\n",
    "    df['primary_category'] = df['categories'].map(dict_)\n",
    "    df.drop(['categories'], axis =1, inplace=True)\n",
    "    return df\n",
    "\n",
    "map_main_category(business_df, category_dict).head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=3></a> 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id=3.1></a>3.1 Visualize star rating distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Get the distribution of the ratings\n",
    "x = business_df['business_stars'].value_counts().sort_index()\n",
    "#plot\n",
    "plt.figure(figsize=(15, 8))\n",
    "ax = sns.barplot(x.index, x.values, alpha=0.8)\n",
    "plt.title(\"Star Rating Distribution\")\n",
    "plt.ylabel('# of businesses', fontsize=12)\n",
    "plt.xlabel('Star Ratings ', fontsize=12)\n",
    "\n",
    "#adding the text labels\n",
    "rects = ax.patches\n",
    "labels = x.values\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id=3.2></a>3.2 Visualize category distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = business_df.primary_category.value_counts()\n",
    "print(\"There are \",len(x),\" different types/categories of Businesses in Yelp.\")\n",
    "count = 0\n",
    "for index, num in enumerate(x.iloc[0:20]):\n",
    "    count += num\n",
    "print('Number of business in Top20: {}'.format(count))\n",
    "\n",
    "#prep for chart\n",
    "x = x.sort_values(ascending = False)\n",
    "top_categories = x.index.tolist()\n",
    "\n",
    "#chart\n",
    "plt.figure(figsize = (16,8))\n",
    "ax = sns.barplot(x.index, x.values, alpha = 0.8)\n",
    "plt.title(\"Category Distribution\",fontsize = 25)\n",
    "locs, labels = plt.xticks()\n",
    "plt.setp(labels, rotation = 80)\n",
    "plt.ylabel('# businesses', fontsize = 12)\n",
    "plt.xlabel('Category', fontsize = 12)\n",
    "\n",
    "#adding the text labels\n",
    "rects = ax.patches\n",
    "labels = x.values\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha = 'center', va = 'bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id=3.3></a> 3.3 Visualize Review Counts by City Distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = business_df['city'].value_counts().sort_values(ascending=False).iloc[0:20]\n",
    "plt.figure(figsize=(16,8))\n",
    "ax = sns.barplot(x.index, x.values, alpha=0.8)\n",
    "plt.title(\"Review Count by City\")\n",
    "locs, labels = plt.xticks()\n",
    "plt.setp(labels, rotation=45)\n",
    "plt.ylabel('# businesses', fontsize=12)\n",
    "plt.xlabel('City', fontsize=12)\n",
    "\n",
    "#adding the text labels\n",
    "rects = ax.patches\n",
    "labels = x.values\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha = 'center', va = 'bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=4></a> 4. Data Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('business+business_hours df shape: {}'.format(business_df.shape))\n",
    "print('review df shape: {}'.format(review_df.shape))\n",
    "print('tip df shape: {}'.format(tip_df.shape))\n",
    "print('checkin df shape: {}'.format(checkin_df.shape))\n",
    "print('user df shape: {}'.format(user_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dfs = [business_df, tip_df, checkin_df, review_df]\n",
    "# df2 = reduce(lambda left,right: pd.merge(left,right,on='business_id'), dfs)\n",
    "# maindf = pd.merge(df2, user_df, how='inner', on='user_id')\n",
    "# maindf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#join tables\n",
    "testdf = pd.merge(business_df, tip_df, how='inner', on='business_id')\n",
    "testdf2 = pd.merge(testdf, checkin_df, how='inner', on='business_id')\n",
    "testdf3 = pd.merge(testdf2, review_df, how='inner', on='business_id')\n",
    "maindf = pd.merge(testdf3, user_df, how='inner', on='user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maindf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maindf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maindf.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=5></a>5. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id=5.1></a> 5.1 Calculate number of business days within a week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "weekdays = ['monday', 'tuesday', 'wednesday', 'thursday', 'friday']\n",
    "weekends = ['saturday', 'sunday']\n",
    "\n",
    "def business_days(df):\n",
    "    weekdays = ['monday', 'tuesday', 'wednesday', 'thursday', 'friday']\n",
    "    weekends = ['saturday', 'sunday']\n",
    "    temp = df[weekdays+weekends].replace('None', np.nan)\n",
    "    df['number_of_business_days'] = 7 - temp.isnull().sum(axis = 1)\n",
    "    return df\n",
    "\n",
    "business_days(maindf).sample(1, random_state = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#how many business on list has no records of opening\n",
    "print('Total records with 0 business hours: {}'.format(len(maindf.loc[maindf.number_of_business_days == 0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#how many business on list that is not open?\n",
    "print('Total records shown as not open: {}'.format(len(maindf.loc[maindf.is_open == 0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id = 5.2></a> 5.2 Calculate number of business hours within a week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def business_hours(df):\n",
    "    days = ['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday']\n",
    "    store_hours = pd.DataFrame()\n",
    "\n",
    "    for i in range(len(days)):\n",
    "        temp = pd.DataFrame(df[days[i]].replace(to_replace='None', value='0-0'))\n",
    "        temp[['t1','t2']] = pd.DataFrame(temp[days[i]].str.split('-').values.tolist(), index = temp.index)\n",
    "        temp['hh1'], temp['mm1'] = temp['t1'].str.split(':', 1).str\n",
    "        temp['hh2'], temp['mm2'] = temp['t2'].str.split(':', 1).str\n",
    "        temp['hours'] = temp['hh2'].astype(int) - temp['hh1'].astype(int)\n",
    "        temp.hours[temp.hours < 0] = 24 - temp.hh1.astype(int) + temp.hh2.astype(int)\n",
    "        store_hours[i] = temp.hours\n",
    "    \n",
    "    df['total_business_hours'] = store_hours.sum(axis=1)\n",
    "    df.drop(days, axis = 1, inplace = True)\n",
    "    \n",
    "    return df\n",
    "business_hours(maindf).head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id=5.3></a> 5.3 Extract the length of the business name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def business_name_length(df):\n",
    "    df['business_name_length'] = df['business_name'].apply(lambda x:len(x.split(' ')))\n",
    "    return df\n",
    "\n",
    "business_name_length(maindf).head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id=5.4></a> 5.4 Extract number of vowels in the business name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vowels_in_business_name(df):\n",
    "    vow_list = []\n",
    "    for title in df.business_name:\n",
    "        vow = 0\n",
    "        word = title.split(' ')\n",
    "        for w in word:\n",
    "            ws = w.lstrip('\"').rstrip('\"')\n",
    "            vow += sum(letter in 'aeiou' for letter in ws.lower())\n",
    "        vow_list.append(vow)\n",
    "\n",
    "    df['number_of_vowels_in_business_name'] = vow_list\n",
    "    return df\n",
    "\n",
    "vowels_in_business_name(maindf).head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id=5.5></a> 5.5 Extract number of friends on Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def number_of_friends(df):\n",
    "    df['number_of_friends'] = df.friends.replace('None', np.nan).str.split(', ').str.len()\n",
    "    df['number_of_friends'] = df['number_of_friends'].fillna(0.0).astype(int)\n",
    "    df.drop(['friends'], axis = 1)\n",
    "    return df\n",
    "\n",
    "number_of_friends(maindf).head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id=5.6></a> 5.6 Final data cleaning prior to modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def drop_cols_post_feature_engineering(df):\n",
    "    drop_final_maindfcol = ['user_id', 'user_id_y', 'review_id', 'business_id', 'friends']\n",
    "    df.drop(drop_final_maindfcol, axis = 1, inplace = True)\n",
    "    return df\n",
    "\n",
    "drop_cols_post_feature_engineering(maindf).head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=6></a> 6. Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id=6.1></a> 6.1 Define classification metrics function & downcasting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yelp_df = maindf.copy()\n",
    "yelp_df.fillna('Others',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reset dtypes\n",
    "temp_dtypes = {\n",
    "    'business_stars': np.float32,\n",
    "    'review_count': np.int32,\n",
    "    'is_open': np.int8,\n",
    "    'tip_count': np.int32,\n",
    "    'checkins': np.int32,\n",
    "    'stars': np.int32,\n",
    "    'r_review': np.int32,\n",
    "    'r_funny': np.int32,\n",
    "    'r_cool': np.int32,\n",
    "    'user_review_counts': np.int32,\n",
    "    'u_useful': np.int32,\n",
    "    'u_funny': np.int32,\n",
    "    'u_cool': np.int32,\n",
    "    'u_fans': np.int32,\n",
    "    'average_stars': np.float32,\n",
    "    'compliment_cute': np.int32,\n",
    "    'compliment_more': np.int32,\n",
    "    'compliment_profile': np.int32,\n",
    "    'compliment_cute': np.int32,\n",
    "    'compliment_list': np.int32,\n",
    "    'compliment_note': np.int32,\n",
    "    'compliment_plain': np.int32,\n",
    "    'compliment_cool': np.int32,\n",
    "    'compliment_funny': np.int32,\n",
    "    'compliment_writer': np.int32,\n",
    "    'compliment_photos': np.int32,\n",
    "    'number_of_business_days': np.int32,\n",
    "    'total_business_hours': np.int32,\n",
    "    'business_name_length': np.int32,\n",
    "    'number_of_vowels_in_business_name': np.int32,\n",
    "    'number_of_friends': np.int32,\n",
    "    }\n",
    "\n",
    "for col, col_type in temp_dtypes.items():\n",
    "    yelp_df[col] = yelp_df[col].astype(col_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classification_loops(df, x, y, model_list, model_names_list):\n",
    "    scores_table = pd.DataFrame()\n",
    "    scores = []\n",
    "    baseline_xtrain, baseline_xtest, baseline_ytrain, baseline_ytest = train_test_split(x, y, \n",
    "                                                                                        test_size=0.2,random_state=42)\n",
    "\n",
    "    #get scores\n",
    "    for i, model in enumerate(model_list):\n",
    "        scores.append(np.mean(cross_val_score(model, baseline_xtrain, baseline_ytrain, cv=3, \n",
    "                                              scoring=make_scorer(metrics.accuracy_score))))\n",
    "        print(i)\n",
    "        scores.append(np.mean(cross_val_score(model, baseline_xtrain, baseline_ytrain, cv=3, \n",
    "                                              scoring=make_scorer(metrics.precision_score, average='macro'))))\n",
    "        print(i)\n",
    "        scores.append(np.mean(cross_val_score(model, baseline_xtrain, baseline_ytrain, cv=3, \n",
    "                                              scoring=make_scorer(metrics.recall_score, average='macro'))))\n",
    "        print(i)\n",
    "        scores.append(np.mean(cross_val_score(model, baseline_xtrain, baseline_ytrain, cv=3, \n",
    "                                              scoring=make_scorer(metrics.f1_score, average='macro'))))\n",
    "        print(i)\n",
    "    feature_scores = pd.DataFrame(np.array(scores).reshape(len(model_list), 4), \n",
    "                         columns=['Accuracy', 'Precision', 'Recall', 'F1'], index=model_names_list)\n",
    "\n",
    "    scores_table = pd.concat([scores_table, feature_scores])\n",
    "    return scores_table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id=6.2></a> 6.2 Baseline Logistic Regreesion model (One feature / 5 feature(non Feature-Eng) / 5 feature (Feature-Eng) / 10 feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define baseline model features and models\n",
    "blv1_x = yelp_df[['review_count']]\n",
    "blv1_y = yelp_df.primary_category\n",
    "baseline_xtrain, baseline_xtest, baseline_ytrain, baseline_ytest = train_test_split(blv1_x, blv1_y, \n",
    "                                                                                     test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yelp_lr = LogisticRegression(n_jobs=-1)\n",
    "yelp_baseline_lr = yelp_lr.fit(baseline_xtrain, baseline_ytrain)\n",
    "yelp_baseline_lr_pred = yelp_baseline_lr.predict(baseline_xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('baseline logistic regression coefficient:')\n",
    "list(zip(yelp_baseline_lr.classes_, yelp_baseline_lr.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yelp_lrtpr, yelp_lrfpr, yelp_thresh_roc = roc_curve(np.array(baseline_ytest), \n",
    "                                                    yelp_baseline_lr.predict_proba(baseline_xtest)[:,1], \n",
    "                                                    pos_label='Pets')\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.plot(yelp_lrfpr, yelp_lrtpr, color='#ff6666')\n",
    "plt.plot([0,1], [0,1],linestyle='--',color='gray')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.text(0.1, 0.9,'AUC: {:0.3f}'.format(metrics.auc(yelp_lrfpr, yelp_lrtpr)), fontsize=15)\n",
    "plt.title('ROC curve for Yelp Baseline Logistic Regression');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('lr_baseline accuracy: {}'.format(np.mean(cross_val_score(yelp_lr, blv1_x, blv1_y, \n",
    "                                                           cv=3, scoring=make_scorer(metrics.accuracy_score)))))\n",
    "print('lr_baseline precision: {}'.format(np.mean(cross_val_score(yelp_lr, blv1_x, blv1_y, \n",
    "                                                           cv=3, scoring=make_scorer(metrics.precision_score, average='macro')))))\n",
    "print('lr_baseline recall: {}'.format(np.mean(cross_val_score(yelp_lr, blv1_x, blv1_y, \n",
    "                                                           cv=3, scoring=make_scorer(metrics.recall_score, average='macro')))))\n",
    "print('lr_baseline F1: {}'.format(np.mean(cross_val_score(yelp_lr, blv1_x, blv1_y, \n",
    "                                                           cv=3, scoring=make_scorer(metrics.f1_score, average='macro')))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr_baseline_cnf = confusion_matrix(baseline_ytest,yelp_baseline_lr_pred)\n",
    "plot_confusion_matrix(lr_baseline_cnf, classes=list(set(yelp_df.primary_category.tolist())), title='Logitic Baseline CM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id=6.3></a> 6.3 Expanding Logistic Regression with 5 original features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define baseline model with 5 features\n",
    "blv1_5x = yelp_df[['review_count', 'business_stars', 'review_count', 'stars', 'average_stars']]\n",
    "blv1_5y = yelp_df.primary_category\n",
    "baseline5f_xtrain, baseline5f_xtest, baseline5f_ytrain, baseline5f_ytest = train_test_split(blv1_5x, blv1_5y, \n",
    "                                                                                     test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yelp_lr5f = LogisticRegression(penalty='l1', n_jobs=-1)\n",
    "yelp_baseline5f_lr = yelp_lr5f.fit(baseline5f_xtrain, baseline5f_ytrain)\n",
    "yelp_baseline5f_lr_pred = yelp_baseline5f_lr.predict(baseline5f_xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('baseline (5feature) logistic regression coefficient:')\n",
    "list(zip(yelp_baseline5f_lr.classes_, yelp_baseline5f_lr.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('lr_baseline (5e) recall: {}'.format(np.mean(cross_val_score(yelp_baseline5f_lr, blv1_5x, blv1_5y, \n",
    "                                                           cv=3, scoring=make_scorer(metrics.recall_score, average='macro')))))\n",
    "print('lr_baseline (5e) F1: {}'.format(np.mean(cross_val_score(yelp_baseline5f_lr, blv1_5x, blv1_5y, \n",
    "                                                           cv=3, scoring=make_scorer(metrics.f1_score, average='macro')))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id=6.4></a> 6.4 Expanding Logistic Regression with 5 feature engineered features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#5 feature baseline with engineering features\n",
    "blv1_5xf = yelp_df[['review_count', 'number_of_vowels_in_business_name', 'business_name_length', 'total_business_hours', 'number_of_business_days']]\n",
    "blv1_5yf = yelp_df.primary_category\n",
    "baseline5fe_xtrain, baseline5fe_xtest, baseline5fe_ytrain, baseline5fe_ytest = train_test_split(blv1_5xf, blv1_5yf, \n",
    "                                                                                     test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yelp_lr5fe = LogisticRegression(penalty='l1', n_jobs=-1)\n",
    "yelp_baseline5fe_lr = yelp_lr5fe.fit(baseline5f_xtrain, baseline5fe_ytrain)\n",
    "yelp_baseline5fe_lr_pred = yelp_baseline5fe_lr.predict(baseline5fe_xtest)\n",
    "\n",
    "print('baseline logistic regression coefficient:')\n",
    "list(zip(yelp_baseline5fe_lr.classes_, yelp_baseline5fe_lr.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('lr_baseline recall: {}'.format(np.mean(cross_val_score(yelp_baseline5fe_lr, blv1_5x, blv1_5y, \n",
    "                                                           cv=3, scoring=make_scorer(metrics.recall_score, average='macro')))))\n",
    "print('lr_baseline F1: {}'.format(np.mean(cross_val_score(yelp_baseline5fe_lr, blv1_5x, blv1_5y, \n",
    "                                                           cv=3, scoring=make_scorer(metrics.f1_score, average='macro')))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id=6.5></a> 6.5 Expanding Logistic Regression with 10 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#baseline + 10 features\n",
    "blv1_10xf = yelp_df[['review_count', 'number_of_vowels_in_business_name', 'business_name_length', 'total_business_hours', 'number_of_business_days',\n",
    "                    'tip_count', 'business_stars', 'review_count', 'stars', 'average_stars']]\n",
    "blv1_10yf = yelp_df.primary_category\n",
    "baseline10fe_xtrain, baseline10fe_xtest, baseline10fe_ytrain, baseline10fe_ytest = train_test_split(blv1_10xf, blv1_10yf, \n",
    "                                                                                     test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yelp_lr10fe = LogisticRegression(penalty='l1', n_jobs=-1)\n",
    "yelp_baseline10fe_lr = yelp_lr10fe.fit(baseline10fe_xtrain, baseline10fe_ytrain)\n",
    "yelp_baseline10fe_lr_pred = yelp_baseline10fe_lr.predict(baseline10fe_xtest)\n",
    "\n",
    "print('baseline logistic regression coefficient:')\n",
    "list(zip(yelp_baseline10fe_lr.classes_, yelp_baseline10fe_lr.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('lr_baseline_10f recall: {}'.format(np.mean(cross_val_score(yelp_baseline10fe_lr, blv1_10xf, blv1_10yf, \n",
    "                                                           cv=3, scoring=make_scorer(metrics.recall_score, average='macro')))))\n",
    "print('lr_baseline_10f F1: {}'.format(np.mean(cross_val_score(yelp_baseline10fe_lr, blv1_10xf, blv1_10yf, \n",
    "                                                           cv=3, scoring=make_scorer(metrics.f1_score, average='macro')))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a =id=6.6></a> 6.6 Logistic Regression Model + all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define baseline model features and models\n",
    "v2_x = yelp_df.drop(['business_name', 'primary_category', 'review_text', 'user_name', 'city', 'state'], axis = 1)\n",
    "v2_y = yelp_df.primary_category\n",
    "v2_xtrain, v2_xtest, v2_ytrain, v2_ytest = train_test_split(v2_x, v2_y, test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yelp_lrv2 = LogisticRegression(n_jobs=-1)\n",
    "yelp_v2_lr = yelp_lrv2.fit(v2_xtrain, v2_ytrain)\n",
    "yelp_v2_lr_pred = yelp_v2_lr.predict(v2_xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('V2_all_feature logistic regression coefficient:')\n",
    "list(zip(yelp_v2_lr.classes_, yelp_v2_lr.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('lrv2_allfeature recall: {}'.format(np.mean(cross_val_score(yelp_lrv2, v2_x, v2_y, \n",
    "                                                           cv=3, scoring=make_scorer(metrics.recall_score, average='macro')))))\n",
    "print('lrv2_allfeature F1: {}'.format(np.mean(cross_val_score(yelp_lrv2, v2_x, v2_y, \n",
    "                                                           cv=3, scoring=make_scorer(metrics.f1_score, average='macro')))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr_v2_cnf = confusion_matrix(v2_ytest, yelp_v2_lr_pred)\n",
    "plot_confusion_matrix(lr_v2_cnf, classes=list(set(yelp_df.primary_category.tolist())), title='Logistic all_feature CM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id=6.7></a> 6.7 Baseline RF model (One feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blv1_x = yelp_df[['review_count']]\n",
    "blv1_y = yelp_df.primary_category\n",
    "baseline_xtrain, baseline_xtest, baseline_ytrain, baseline_ytest = train_test_split(blv1_x, blv1_y, \n",
    "                                                                                     test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yelp_RF = RandomForestClassifier(n_estimators = 1000, n_jobs=-1)\n",
    "yelp_baseline_RF = yelp_RF.fit(baseline_xtrain, baseline_ytrain)\n",
    "yelp_baseline_RF_pred = yelp_baseline_RF.predict(baseline_xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yelp_RFtpr, yelp_RFfpr, yelp_RFthresh_roc = roc_curve(np.array(baseline_ytest), \n",
    "                                                    yelp_baseline_RF.predict_proba(baseline_xtest)[:,1], \n",
    "                                                    pos_label='Pets')\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.plot(yelp_RFfpr, yelp_RFtpr, color='#ff6666')\n",
    "plt.plot([0,1], [0,1],linestyle='--',color='gray')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.text(0.1, 0.9,'AUC: {:0.3f}'.format(metrics.auc(yelp_RFfpr, yelp_RFtpr)), fontsize=15)\n",
    "plt.title('ROC curve for Yelp Baseline RF');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('RF_baseline accuracy: {}'.format(np.mean(cross_val_score(yelp_baseline_RF, blv1_x, blv1_y, \n",
    "                                                           cv=3, scoring=make_scorer(metrics.accuracy_score)))))\n",
    "print('RF_baseline precision: {}'.format(np.mean(cross_val_score(yelp_baseline_RF, blv1_x, blv1_y, \n",
    "                                                           cv=3, scoring=make_scorer(metrics.precision_score, average='macro')))))\n",
    "print('RF_baseline recall: {}'.format(np.mean(cross_val_score(yelp_baseline_RF, blv1_x, blv1_y, \n",
    "                                                           cv=3, scoring=make_scorer(metrics.recall_score, average='macro')))))\n",
    "print('RF_baseline F1: {}'.format(np.mean(cross_val_score(yelp_baseline_RF, blv1_x, blv1_y, \n",
    "                                                           cv=3, scoring=make_scorer(metrics.f1_score, average='macro')))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RF_baseline_cnf = confusion_matrix(baseline_ytest, yelp_baseline_RF_pred)\n",
    "plot_confusion_matrix(RF_baseline_cnf, classes=list(set(yelp_df.primary_category.tolist())), title='RF baseline CM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id=6.8></a> 6.8 Baseline xgBoost model (One feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blv1_x = yelp_df[['review_count']]\n",
    "blv1_y = yelp_df.primary_category\n",
    "baseline_xtrain, baseline_xtest, baseline_ytrain, baseline_ytest = train_test_split(blv1_x, blv1_y, \n",
    "                                                                                     test_size=0.2,random_state=42)\n",
    "#baseline_xtrain, baseline_xval, baseline_ytrain, baseline_yval = train_test_split(baseline_xtrain, baseline_ytrain, \n",
    "#                                                                  test_size=0.25, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yelp_gbm = xgb.XGBClassifier(n_estimators=10000, max_depth=4, objective='multi:softmax',\n",
    "                        learning_rate=.05, subsample=.8, min_child_weight=3,\n",
    "                        colsample_bytree=.8, n_jobs=-1)\n",
    "yelp_baseline_gbm = yelp_gbm.fit(baseline_xtrain, baseline_ytrain, \n",
    "                    eval_set=[(baseline_xtrain, baseline_ytrain),(baseline_xval, baseline_yval)],\n",
    "                    eval_metric='merror', early_stopping_rounds=50, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f1_score(baseline_ytest, yelp_gbm.predict(baseline_xtest, ntree_limit=yelp_gbm.best_ntree_limit), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recall_score(baseline_ytest, yelp_gbm.predict(baseline_xtest, ntree_limit=yelp_gbm.best_ntree_limit), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "xgb_tuned_param=[{'learning_rate':[.01,.1,1,10,.001],'max_depth':[3,4,5,6,7,8]}]\n",
    "clf = GridSearchCV(yelp_baseline_gbm, xgb_tuned_param, cv=3, scoring='f1')\n",
    "clf.fit(baseline_xtrain, baseline_ytrain)\n",
    "\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(clf.best_estimator_)\n",
    "print()\n",
    "print(\"Grid scores on development set:\")\n",
    "print()\n",
    "for params, mean_score, scores in clf.grid_scores_:\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "             % (mean_score, scores.std() / 2, params))\n",
    "    \n",
    "#0.118 (+/-0.002) for {'learning_rate': 0.05, 'max_depth': 4, 'objective': 'multi:softmax'}\n",
    "#0.155 (+/-0.000) for {'learning_rate': 0.05, 'max_depth': 8, 'objective': 'multi:softmax'}\n",
    "#0.160 (+/-0.000) for {'learning_rate': 0.05, 'max_depth': 9, 'objective': 'multi:softmax'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('RFv2_allfeature recall: {}'.format(np.mean(cross_val_score(yelp_baseline_gbm, blv1_x, blv1_y, \n",
    "                                                           cv=3, scoring=make_scorer(metrics.recall_score, average='macro')))))\n",
    "print('RFv2_allfeature F1: {}'.format(np.mean(cross_val_score(yelp_baseline_gbm, blv1_x, blv1_y, \n",
    "                                                           cv=3, scoring=make_scorer(metrics.f1_score, average='macro')))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id=6.9></a> 6.9 RF model (All features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define baseline model features and models\n",
    "v2_x = yelp_df.drop(['business_name', 'primary_category', 'review_text', 'user_name', 'city', 'state'], axis = 1)\n",
    "v2_y = yelp_df.primary_category\n",
    "v2_xtrain, v2_xtest, v2_ytrain, v2_ytest = train_test_split(v2_x, v2_y, test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('max feature is determined through taking sqrt of total number of features: {}'\\\n",
    "      .format(math.ceil(np.sqrt(len(v2_xtrain.columns)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yelpv2_RF = RandomForestClassifier(n_estimators = 1000, max_features = math.ceil(np.sqrt(len(v2_xtrain.columns))),\n",
    "                                min_samples_leaf = 4, n_jobs=-1)\n",
    "yelp_v2_RF = yelpv2_RF.fit(v2_xtrain, v2_ytrain)\n",
    "yelp_v2_RF_pred = yelp_v2_RF.predict(v2_xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('RFv2_allfeature accuracy: {}'.format(np.mean(cross_val_score(yelpv2_RF, v2_x, v2_y, \n",
    "                                                           cv=3, scoring=make_scorer(metrics.accuracy_score)))))\n",
    "print('RFv2_allfeature precision: {}'.format(np.mean(cross_val_score(yelpv2_RF, v2_x, v2_y, \n",
    "                                                           cv=3, scoring=make_scorer(metrics.precision_score, average='macro')))))\n",
    "print('RFv2_allfeature recall: {}'.format(np.mean(cross_val_score(yelpv2_RF, v2_x, v2_y, \n",
    "                                                           cv=3, scoring=make_scorer(metrics.recall_score, average='macro')))))\n",
    "print('RFv2_allfeature F1: {}'.format(np.mean(cross_val_score(yelpv2_RF, v2_x, v2_y, \n",
    "                                                           cv=3, scoring=make_scorer(metrics.f1_score, average='macro')))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RFv2_feature_importances = sorted(zip(v2_x.columns,abs(yelp_v2_RF.feature_importances_)), key=lambda x: -x[1])[:25]\n",
    "RFv2_feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "\n",
    "plt.figure(figsize=(13,10))\n",
    "features_, scores_ = zip(*RFv2_feature_importances)\n",
    "sns.barplot(y=list(features_), x=list(scores_), palette='coolwarm')\n",
    "plt.title(\"Feature Importances for RF_All_Feature Model\")\n",
    "plt.ylabel('Relative Importances', fontsize=12)\n",
    "plt.xlabel('Features', fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.set_context('notebook')\n",
    "RF_v2_cnf = confusion_matrix(v2_ytest, yelp_v2_RF_pred)\n",
    "plot_confusion_matrix(RF_v2_cnf, classes=list(set(yelp_df.primary_category.tolist())), title='RF all_feature CM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id=6.10></a> 6.10 xgBoost model (All feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v2_x = yelp_df.drop(['business_name', 'primary_category', 'review_text', 'user_name', 'city', 'state', ], axis = 1)\n",
    "v2_y = yelp_df.primary_category\n",
    "v2_xtrain, v2_xtest, v2_ytrain, v2_ytest = train_test_split(v2_x, v2_y, test_size=0.2,random_state=42)\n",
    "v2_xtrain, v2_xval, v2_ytrain, v2_yval = train_test_split(v2_xtrain, v2_ytrain, test_size=0.25, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v2_xtrain.drop(v2_xtrain.columns[15], axis=1, inplace=True)\n",
    "v2_xtest.drop(v2_xtest.columns[15], axis=1, inplace=True)\n",
    "v2_xval.drop(v2_xval.columns[15], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yelpv2_gbm = xgb.XGBClassifier(n_estimators=10000, max_depth=4, objective='multi:softmax',\n",
    "                        learning_rate=0.1, subsample=.8, min_child_weight=3,\n",
    "                        colsample_bytree=.8, n_jobs=-1)\n",
    "yelp_v2_gbm = yelpv2_gbm.fit(v2_xtrain, v2_ytrain, \n",
    "                    eval_set=[(v2_xtrain, v2_ytrain),(v2_xval, v2_yval)],eval_metric='merror', early_stopping_rounds=30, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recall_score(v2_ytest, yelpv2_gbm.predict(v2_xtrain, ntree_limit=yelpv2_gbm.best_ntree_limit), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f1_score(v2_ytest, yelpv2_gbm.predict(v2_xtrain, ntree_limit=yelpv2_gbm.best_ntree_limit), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "v2_xtrain, v2_xtest, v2_ytrain, v2_ytest = train_test_split(v2_x, v2_y, test_size=0.2,random_state=42)\n",
    "xgb_tuned_param=[{'learning_rate':[.01,.1,.001],'max_depth':[3,5,6,7,8], 'objective':['multi:softmax']}]\n",
    "clf = GridSearchCV(xgb.XGBClassifier(), xgb_tuned_param, cv=2, scoring=make_scorer(metrics.f1_score, average='macro'))\n",
    "clf.fit(v2_xtrain, v2_ytrain)\n",
    "\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(clf.best_estimator_)\n",
    "print()\n",
    "print(\"Grid scores on development set:\")\n",
    "print()\n",
    "for params, mean_score, scores in clf.grid_scores_:\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "             % (mean_score, scores.std() / 2, params))\n",
    "\n",
    "    \n",
    "#0.118 (+/-0.002) for {'learning_rate': 0.05, 'max_depth': 4, 'objective': 'multi:softmax'}\n",
    "#0.155 (+/-0.000) for {'learning_rate': 0.05, 'max_depth': 8, 'objective': 'multi:softmax'}\n",
    "#0.160 (+/-0.000) for {'learning_rate': 0.05, 'max_depth': 9, 'objective': 'multi:softmax'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=7></a> 7. Natural Language Processing (NLP) as additional features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id=7.1></a> 7.1 TFIDFVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pre_vec_df = yelp_df.copy()\n",
    "pre_vec_df.review_text = pre_vec_df.review_text.str.lower()\n",
    "pre_vec_df.review_text.replace(r'([^a-z\\s])', '', regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "vectorizer = TfidfVectorizer(stop_words=['a', 'an', 'and', 'are', 'as', 'at', 'be', \n",
    "                                         'by', 'for', 'from', 'has', 'he', 'in', 'is', \n",
    "                                         'its', 'it', 'of', 'on', 'that', 'the',\n",
    "                                         'to', 'was', 'were', 'will', 'with', \n",
    "                                         'she', 'mm', 'off', '-', '&', '...', '!', '\\n', 'du', 'et',\n",
    "                                         'le', 'las'], min_df = 2, max_features = 1000)\n",
    "project_tfidf_vec = vectorizer.fit_transform(pre_vec_df.review_text).toarray()\n",
    "project_tfidf_df = pd.DataFrame(project_tfidf_vec, columns=list(vectorizer.vocabulary_.keys()))\n",
    "project_tfidf_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yelp_tfidf_df = pd.concat([pre_vec_df, project_tfidf_df], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id=7.2></a> 7.2 RF Model (TFIDF + all features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v3_x = yelp_tfidf_df.drop(['business_name', 'primary_category', 'review_text', 'user_name', 'city', 'state'], axis = 1)\n",
    "v3_y = yelp_tfidf_df.primary_category\n",
    "v3_xtrain, v3_xtest, v3_ytrain, v3_ytest = train_test_split(v3_x, v3_y, test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('max feature is determined through taking sqrt of total number of features: {}'\\\n",
    "      .format(math.ceil(np.sqrt(len(v3_xtrain.columns)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "yelpv3_RF = RandomForestClassifier(n_estimators = 500, max_features = 4,\n",
    "                                min_samples_leaf = 1, n_jobs=-1)\n",
    "yelp_v3_RF = yelpv3_RF.fit(v3_xtrain, v3_ytrain)\n",
    "yelp_v3_RF_pred = yelp_v3_RF.predict(v3_xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "rf_tuned_param=[{'n_estimators':[500],'max_features':[10], 'min_samples_leaf':[10]}]\n",
    "rfclf = GridSearchCV(RandomForestClassifier(), rf_tuned_param, cv=3, scoring=make_scorer(metrics.f1_score, average='macro'))\n",
    "rfclf.fit(v3_xtrain, v3_ytrain)\n",
    "\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(rfclf.best_estimator_)\n",
    "print()\n",
    "print(\"Grid scores on development set:\")\n",
    "print()\n",
    "for params, mean_score, scores in rfclf.grid_scores_:\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "             % (mean_score, scores.std() / 2, params))\n",
    "    \n",
    "#0.118 (+/-0.002) for {'learning_rate': 0.05, 'max_depth': 4, 'objective': 'multi:softmax'}\n",
    "#0.155 (+/-0.000) for {'learning_rate': 0.05, 'max_depth': 8, 'objective': 'multi:softmax'}\n",
    "#0.160 (+/-0.000) for {'learning_rate': 0.05, 'max_depth': 9, 'objective': 'multi:softmax'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RFv3_feature_importances = sorted(zip(v3_x.columns,abs(yelp_v3_RF.feature_importances_)), key=lambda x: -x[1])[:25]\n",
    "RFv3_feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "\n",
    "plt.figure(figsize=(13,10))\n",
    "features_, scores_ = zip(*RFv3_feature_importances)\n",
    "sns.barplot(y=list(features_), x=list(scores_), palette='coolwarm')\n",
    "plt.title(\"Feature Importances for RFv3_allfeature+tfidf Model\")\n",
    "plt.ylabel('Features', fontsize=12)\n",
    "plt.xlabel('Relative Importances', fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.set_context('notebook')\n",
    "RF_v3_cnf = confusion_matrix(v3_ytest, yelp_v3_RF_pred)\n",
    "plot_confusion_matrix(RF_v3_cnf, classes=list(set(yelp_tfidf_df.primary_category.tolist())), title='RF all_feature+tfidf CM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print('RFv3_allfeature+tfidf recall: {}'.format(np.mean(cross_val_score(yelp_v3_RF, v3_x, v3_y, \n",
    "#                                                           cv=3, scoring=make_scorer(metrics.recall_score, average='macro')))))\n",
    "print('RFv3_allfeature+tfidf F1: {}'.format(np.mean(cross_val_score(yelp_v3_RF, v3_x, v3_y, \n",
    "                                                           cv=3, scoring=make_scorer(metrics.f1_score, average='macro')))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=8></a> 8. Final Model Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "xgb_tuned_param=[{'learning_rate':[0.05, 1],'max_depth':[4], 'max_depth':[8], 'objective':['multi:softmax']}]\n",
    "clf_final = GridSearchCV(xgb.XGBClassifier(), xgb_tuned_param, cv=2, scoring=make_scorer(metrics.f1_score, average='macro'))\n",
    "clf_final.fit(v2_xtrain, v2_ytrain)\n",
    "\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(clf_final.best_estimator_)\n",
    "print()\n",
    "print(\"Grid scores on development set:\")\n",
    "print()\n",
    "for params, mean_score, scores in clf_final.grid_scores_:\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "             % (mean_score, scores.std() / 2, params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=9></a> 9. Final test-scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vf_x = yelp_df.drop(['business_name', 'primary_category', 'review_text', 'user_name', 'city', 'state', ], axis = 1)\n",
    "vf_y = yelp_df.primary_category\n",
    "vf_xtrain, vf_xtest, vf_ytrain, vf_ytest = train_test_split(vf_x, vf_y, test_size=0.2,random_state=42)\n",
    "vf_xtrain, vf_xval, vf_ytrain, vf_yval = train_test_split(vf_xtrain, vf_ytrain, test_size=0.25, random_state=2)\n",
    "\n",
    "vf_xtrain.drop(vf_xtrain.columns[15], axis=1, inplace=True)\n",
    "vf_xtest.drop(vf_xtest.columns[15], axis=1, inplace=True)\n",
    "vf_xval.drop(vf_xval.columns[15], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yelp_gbm_final = xgb.XGBClassifier(n_estimators=10000, max_depth=4, objective='multi:softmax',\n",
    "                        learning_rate=0.1, subsample=.8, min_child_weight=3,\n",
    "                        colsample_bytree=.8, n_jobs=-1)\n",
    "yelp_gbm_final_model = yelp_gbm_final.fit(vf_xtrain, vf_ytrain, \n",
    "                    eval_set=[(vf_xtrain, vf_ytrain),(vf_xval, vf_yval)],eval_metric='merror', early_stopping_rounds=30, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recall_score(vf_ytest, yelpv2_gbm.predict(vf_xtest, ntree_limit=yelpv2_gbm.best_ntree_limit), average='macro')\n",
    "f1_score(vf_ytest, yelpv2_gbm.predict(vf_xtest, ntree_limit=yelpv2_gbm.best_ntree_limit), average='macro')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
